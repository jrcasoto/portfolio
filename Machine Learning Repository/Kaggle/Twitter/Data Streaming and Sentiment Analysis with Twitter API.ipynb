{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Streaming and Sentiment Analysis with Twitter API (using Python, Spark Streaming and NLP with NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Description\n",
    "This analysis was structured in the following steps:\n",
    "1. Spark Streaming parametrization for Twitter\n",
    "2. Generate text classifier for sentiment analysis\n",
    "3. Twitter API authentication and real-time tweets analysis\n",
    "\n",
    "**NOTE**: For this use case, a virtual machine was setted with **Ubuntu 20.04.3 LTS, Java JDK 1.8** and  **Apache Spark 2.4.2**\n",
    "**NOTEÂ²**: A developer account for Twitter was requested to authentication, but for privacy reasons the credentials used will be masked as 'XXXX'. Outputs will still be recorded so that you can check the entire process end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Streaming parametrization for Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary packages for authentication, twitter API and NLP processing\n",
    "!pip install requests_oauthlib\n",
    "!pip install twython\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK library\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up batch interval update frequency (five seconds)\n",
    "BATCH_INTERVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating StreamingContext\n",
    "ssc = StreamingContext(sc, BATCH_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Text Classifier for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate a text classifier model for sentiment analysis, I've imported a dataset from kaggle **UMICH SI650 - Sentiment Classification** (available in https://inclass.kaggle.com/c/si650winter11; data is also available in .zip format in 'data' folder) with over 1.5mi rows containing evaluations for different tweets with an already implemented machine learning model to predict wether the message has a positive or negative sentiment. \n",
    "This dataset is structured with the following columns:\n",
    "1. **ItemID**: Dataset index (auto incremental);\n",
    "2. **Sentiment**: Binary classificator: **0 for negative and 1 for positive**;\n",
    "3. **SentimentSource**: Source of tweet;\n",
    "4. **SentimentText**: Source with evaluated text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file as an RDD with Spark\n",
    "arquivo = sc.textFile(\"/Users/casoto/Dropbox/DSA/dataset_analise_sentimento.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove header row\n",
    "header = arquivo.take(1)[0]\n",
    "dataset = arquivo.filter(lambda line: line != header)\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to separate values in columns, remove punctuations and generate a tuple with string/sentiment\n",
    "def get_row(line):\n",
    "  row = line.split(',')\n",
    "  sentimento = row[1]\n",
    "  tweet = row[3].strip()\n",
    "  translator = str.maketrans({key: None for key in string.punctuation})\n",
    "  tweet = tweet.translate(translator)\n",
    "  tweet = tweet.split(' ')\n",
    "  tweet_lower = []\n",
    "  for word in tweet:\n",
    "    tweet_lower.append(word.lower())\n",
    "  return (tweet_lower, sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function (lazy evaluation)\n",
    "dataset_train = dataset.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a SentimentAnalyzer object\n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dmpm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloads stopwords library to remove from dataset\n",
    "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
    "nltk.download()\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed list with english stopwords\n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "  stopwords_all.append(word)\n",
    "  stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with first 10k rows\n",
    "dataset_train_sample = dataset_train.take(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from dataset\n",
    "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an unigram (n-gram of size 1) to extract features\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyzer.apply_features(dataset_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.collections.LazyMap"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'contains()': False, 'contains(im)': False, 'contains(_NEG)': False, 'contains(followfriday)': False, 'contains(amp)': False, 'contains(dont)': False, 'contains(day)': False, 'contains(love)': False, 'contains(like)': False, 'contains(cant)': False, 'contains(good)': False, 'contains(get)': False, 'contains(go)': False, 'contains(today)': False, 'contains(got)': False, 'contains(want)': False, 'contains(time)': False, 'contains(going)': False, 'contains(back)': False, 'contains(one)': False, 'contains(sad)': True, 'contains(really)': False, 'contains(miss)': False, 'contains(u)': False, 'contains(work)': False, 'contains(new)': False, 'contains(2)': False, 'contains(last)': False, 'contains(still)': False, 'contains(twitter)': False, 'contains(night)': False, 'contains(great)': False, 'contains(lol)': False, 'contains(follow)': False, 'contains(need)': False, 'contains(see)': False, 'contains(much)': False, 'contains(myweakness)': False, 'contains(get_NEG)': False, 'contains(didnt)': False, 'contains(think)': False, 'contains(hate)': False, 'contains(iremember)': False, 'contains(home)': False, 'contains(feel)': False, 'contains(musicmonday)': False, 'contains(know)': False, 'contains(happy)': False, 'contains(people)': False, 'contains(lt3)': False, 'contains(would)': False, 'contains(bad)': False, 'contains(well)': False, 'contains(right)': False, 'contains(wish)': False, 'contains(oh)': False, 'contains(gonna)': False, 'contains(tomorrow)': False, 'contains(tonight)': False, 'contains(ff)': False, 'contains(ill)': False, 'contains(please)': False, 'contains(hope)': False, 'contains(thanks)': False, 'contains(morning)': False, 'contains(someone)': False, 'contains(never)': False, 'contains(ive)': False, 'contains(make)': False, 'contains(getting)': False, 'contains(im_NEG)': False, 'contains(go_NEG)': False, 'contains(know_NEG)': False, 'contains(awesome)': False, 'contains(like_NEG)': False, 'contains(inaperfectworld)': False, 'contains(thats)': False, 'contains(come)': False, 'contains(squarespace)': False, 'contains(wont)': False, 'contains(haha)': False, 'contains(lt)': False, 'contains(wanna)': False, 'contains(1)': False, 'contains(lost)': False, 'contains(days)': False, 'contains(4)': False, 'contains(makes)': False, 'contains(fun)': False, 'contains(friends)': False, 'contains(life)': False, 'contains(best)': False, 'contains(iphone)': False, 'contains(quoti)': False, 'contains(good_NEG)': False, 'contains(could)': False, 'contains(song)': False, 'contains(school)': False, 'contains(bed)': False, 'contains(better)': False, 'contains(dontyouhate)': False, 'contains(first)': False, 'contains(ever)': False, 'contains(3)': False, 'contains(us)': False, 'contains(haveyouever)': False, 'contains(sick)': False, 'contains(nice)': False, 'contains(man)': False, 'contains(want_NEG)': False, 'contains(doesnt)': False, 'contains(everyone)': False, 'contains(already)': False, 'contains(one_NEG)': False, 'contains(guys)': False, 'contains(show)': False, 'contains(sorry)': False, 'contains(week)': False, 'contains(music)': False, 'contains(things)': False, 'contains(old)': False, 'contains(bgt)': False, 'contains(next)': False, 'contains(made)': False, 'contains(something)': False, 'contains(n)': False, 'contains(way)': False, 'contains(another)': False, 'contains(gt)': False, 'contains(sleep)': False, 'contains(take)': False, 'contains(soon)': False, 'contains(baby)': False, 'contains(isnt)': False, 'contains(work_NEG)': False, 'contains(little)': False, 'contains(away)': False, 'contains(damn)': False, 'contains(quot)': False, 'contains(say)': False, 'contains(always)': False, 'contains(phone)': False, 'contains(left)': False, 'contains(shes)': False, 'contains(see_NEG)': False, 'contains(summer)': False, 'contains(weekend)': False, 'contains(year)': False, 'contains(today_NEG)': False, 'contains(amazing)': False, 'contains(wait_NEG)': False, 'contains(ok)': False, 'contains(long)': False, 'contains(girl)': False, 'contains(world)': False, 'contains(watching)': False, 'contains(movie)': False, 'contains(goodsex)': False, 'contains(feeling)': False, 'contains(ur)': False, 'contains(watch)': False, 'contains(cool)': False, 'contains(found)': False, 'contains(friend)': True, 'contains(mom)': False, 'contains(hes)': False, 'contains(friday)': False, 'contains(done)': False, 'contains(hours)': False, 'contains(said)': False, 'contains(went)': False, 'contains(gone)': False, 'contains(tired)': False, 'contains(house)': False, 'contains(missed)': False, 'contains(give)': False, 'contains(rain)': False, 'contains(leave)': False, 'contains(thing)': False, 'contains(wanted)': False, 'contains(head)': False, 'contains(sucks)': False, 'contains(sleep_NEG)': False, 'contains(ready)': False, 'contains(thank)': False, 'contains(guess)': False, 'contains(nothing)': False, 'contains(talk)': False, 'contains(followers)': False, 'contains(keep)': False, 'contains(tweets)': False, 'contains(look)': False, 'contains(hurts)': False, 'contains(early)': False, 'contains(game)': False, 'contains(two)': False, 'contains(guy)': False, 'contains(cry)': False, 'contains(going_NEG)': False, 'contains(live)': False}, '0'), ({'contains()': False, 'contains(im)': False, 'contains(_NEG)': False, 'contains(followfriday)': False, 'contains(amp)': False, 'contains(dont)': False, 'contains(day)': False, 'contains(love)': False, 'contains(like)': False, 'contains(cant)': False, 'contains(good)': False, 'contains(get)': False, 'contains(go)': False, 'contains(today)': False, 'contains(got)': False, 'contains(want)': False, 'contains(time)': False, 'contains(going)': False, 'contains(back)': False, 'contains(one)': False, 'contains(sad)': False, 'contains(really)': False, 'contains(miss)': False, 'contains(u)': False, 'contains(work)': False, 'contains(new)': True, 'contains(2)': False, 'contains(last)': False, 'contains(still)': False, 'contains(twitter)': False, 'contains(night)': False, 'contains(great)': False, 'contains(lol)': False, 'contains(follow)': False, 'contains(need)': False, 'contains(see)': False, 'contains(much)': False, 'contains(myweakness)': False, 'contains(get_NEG)': False, 'contains(didnt)': False, 'contains(think)': False, 'contains(hate)': False, 'contains(iremember)': False, 'contains(home)': False, 'contains(feel)': False, 'contains(musicmonday)': False, 'contains(know)': False, 'contains(happy)': False, 'contains(people)': False, 'contains(lt3)': False, 'contains(would)': False, 'contains(bad)': False, 'contains(well)': False, 'contains(right)': False, 'contains(wish)': False, 'contains(oh)': False, 'contains(gonna)': False, 'contains(tomorrow)': False, 'contains(tonight)': False, 'contains(ff)': False, 'contains(ill)': False, 'contains(please)': False, 'contains(hope)': False, 'contains(thanks)': False, 'contains(morning)': False, 'contains(someone)': False, 'contains(never)': False, 'contains(ive)': False, 'contains(make)': False, 'contains(getting)': False, 'contains(im_NEG)': False, 'contains(go_NEG)': False, 'contains(know_NEG)': False, 'contains(awesome)': False, 'contains(like_NEG)': False, 'contains(inaperfectworld)': False, 'contains(thats)': False, 'contains(come)': False, 'contains(squarespace)': False, 'contains(wont)': False, 'contains(haha)': False, 'contains(lt)': False, 'contains(wanna)': False, 'contains(1)': False, 'contains(lost)': False, 'contains(days)': False, 'contains(4)': False, 'contains(makes)': False, 'contains(fun)': False, 'contains(friends)': False, 'contains(life)': False, 'contains(best)': False, 'contains(iphone)': False, 'contains(quoti)': False, 'contains(good_NEG)': False, 'contains(could)': False, 'contains(song)': False, 'contains(school)': False, 'contains(bed)': False, 'contains(better)': False, 'contains(dontyouhate)': False, 'contains(first)': False, 'contains(ever)': False, 'contains(3)': False, 'contains(us)': False, 'contains(haveyouever)': False, 'contains(sick)': False, 'contains(nice)': False, 'contains(man)': False, 'contains(want_NEG)': False, 'contains(doesnt)': False, 'contains(everyone)': False, 'contains(already)': False, 'contains(one_NEG)': False, 'contains(guys)': False, 'contains(show)': False, 'contains(sorry)': False, 'contains(week)': False, 'contains(music)': False, 'contains(things)': False, 'contains(old)': False, 'contains(bgt)': False, 'contains(next)': False, 'contains(made)': False, 'contains(something)': False, 'contains(n)': False, 'contains(way)': False, 'contains(another)': False, 'contains(gt)': False, 'contains(sleep)': False, 'contains(take)': False, 'contains(soon)': False, 'contains(baby)': False, 'contains(isnt)': False, 'contains(work_NEG)': False, 'contains(little)': False, 'contains(away)': False, 'contains(damn)': False, 'contains(quot)': False, 'contains(say)': False, 'contains(always)': False, 'contains(phone)': False, 'contains(left)': False, 'contains(shes)': False, 'contains(see_NEG)': False, 'contains(summer)': False, 'contains(weekend)': False, 'contains(year)': False, 'contains(today_NEG)': False, 'contains(amazing)': False, 'contains(wait_NEG)': False, 'contains(ok)': False, 'contains(long)': False, 'contains(girl)': False, 'contains(world)': False, 'contains(watching)': False, 'contains(movie)': False, 'contains(goodsex)': False, 'contains(feeling)': False, 'contains(ur)': False, 'contains(watch)': False, 'contains(cool)': False, 'contains(found)': False, 'contains(friend)': False, 'contains(mom)': False, 'contains(hes)': False, 'contains(friday)': False, 'contains(done)': False, 'contains(hours)': False, 'contains(said)': False, 'contains(went)': False, 'contains(gone)': False, 'contains(tired)': False, 'contains(house)': False, 'contains(missed)': True, 'contains(give)': False, 'contains(rain)': False, 'contains(leave)': False, 'contains(thing)': False, 'contains(wanted)': False, 'contains(head)': False, 'contains(sucks)': False, 'contains(sleep_NEG)': False, 'contains(ready)': False, 'contains(thank)': False, 'contains(guess)': False, 'contains(nothing)': False, 'contains(talk)': False, 'contains(followers)': False, 'contains(keep)': False, 'contains(tweets)': False, 'contains(look)': False, 'contains(hurts)': False, 'contains(early)': False, 'contains(game)': False, 'contains(two)': False, 'contains(guy)': False, 'contains(cry)': False, 'contains(going_NEG)': False, 'contains(live)': False}, '0'), ...]\n"
     ]
    }
   ],
   "source": [
    "# Check unigram for classification\n",
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "# Train model with Naive Bayes Classifier\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classifier within sample phases\n",
    "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
    "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
    "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Twitter API authentication and real-time tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with requests library Twitter API; paste keys\n",
    "consumer_key = \"XXXX\"\n",
    "consumer_secret = \"XXXX\"\n",
    "access_token = \"XXXX\"\n",
    "access_token_secret = \"XXXX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get twitter account as search_term and stream tweets redirected to that specific user's URL (i.e Trump)\n",
    "search_term = 'Trump'\n",
    "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
    "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track='+search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create auth object for Twitter's Developer account\n",
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Spark Streaming context\n",
    "rdd = ssc.sparkContext.parallelize([0])\n",
    "stream = ssc.queueStream([], default = rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.streaming.dstream.DStream"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to 500 tweets (constraint)\n",
    "NUM_TWEETS = 500  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns labeled 'NUM_TWEETS' and append them into json file\n",
    "def tfunc(t, rdd):\n",
    "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "  response = requests.get(filter_url, auth = auth, stream = True)\n",
    "  print(filter_url, response)\n",
    "  count = 0\n",
    "  for line in response.iter_lines():\n",
    "    try:\n",
    "      if count > NUM_TWEETS:\n",
    "        break\n",
    "      post = json.loads(line.decode('utf-8'))\n",
    "      contents = [post['text']]\n",
    "      count += 1\n",
    "      yield str(contents)\n",
    "    except:\n",
    "      result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function\n",
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies text classifier for sentiment analysis for each tweet\n",
    "def classifica_tweet(tweet):\n",
    "  sentence = [(tweet, '')]\n",
    "  test_set = sentiment_analyzer.apply_features(sentence)\n",
    "  print(tweet, classifier.classify(test_set[0][0]))\n",
    "  return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns clean tweet text\n",
    "def get_tweet_text(rdd):\n",
    "  for line in rdd:\n",
    "    tweet = line.strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "      tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output result list to store collected data\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save microbatches according to that specific timestamp \n",
    "def output_rdd(rdd):\n",
    "  global results\n",
    "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "  counts = pairs.reduceByKey(add)\n",
    "  output = []\n",
    "  for count in counts.collect():\n",
    "    output.append(count)\n",
    "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "  resultados.append(result)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachRDD() applies a function for each RDD in a data streaming\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10:40:22', []]\n",
      "['10:40:54', [('0', 372), ('1', 129)]]\n"
     ]
    }
   ],
   "source": [
    "# Start streaming\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10:41:24', [('0', 389), ('1', 112)]]\n",
      "['10:41:55', [('0', 371), ('1', 130)]]\n",
      "['10:42:26', [('1', 131), ('0', 370)]]\n",
      "['10:42:57', [('1', 138), ('0', 363)]]\n",
      "['10:43:29', [('1', 127), ('0', 374)]]\n",
      "['10:43:59', [('0', 375), ('1', 126)]]\n",
      "['10:44:30', [('1', 122), ('0', 379)]]\n",
      "['10:45:01', [('1', 131), ('0', 370)]]\n",
      "['10:45:33', [('0', 368), ('1', 133)]]\n",
      "['10:46:04', [('1', 124), ('0', 377)]]\n",
      "['10:46:35', [('1', 120), ('0', 381)]]\n",
      "['10:47:06', [('0', 387), ('1', 114)]]\n"
     ]
    }
   ],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "  if len(results) > 5:\n",
    "    cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10:47:36', [('0', 386), ('1', 115)]]\n"
     ]
    }
   ],
   "source": [
    "# Record results in folder\n",
    "rdd_save = '/Users/casoto/Dropbox/r'+time.strftime(\"%I%M%S\")\n",
    "res_rdd = sc.parallelize(results)\n",
    "res_rdd.saveAsTextFile(rdd_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['10:40:22', []],\n",
       " ['10:40:54', [('0', 372), ('1', 129)]],\n",
       " ['10:41:24', [('0', 389), ('1', 112)]],\n",
       " ['10:41:55', [('0', 371), ('1', 130)]],\n",
       " ['10:42:26', [('1', 131), ('0', 370)]],\n",
       " ['10:42:57', [('1', 138), ('0', 363)]],\n",
       " ['10:43:29', [('1', 127), ('0', 374)]],\n",
       " ['10:43:59', [('0', 375), ('1', 126)]],\n",
       " ['10:44:30', [('1', 122), ('0', 379)]],\n",
       " ['10:45:01', [('1', 131), ('0', 370)]],\n",
       " ['10:45:33', [('0', 368), ('1', 133)]],\n",
       " ['10:46:04', [('1', 124), ('0', 377)]],\n",
       " ['10:46:35', [('1', 120), ('0', 381)]],\n",
       " ['10:47:06', [('0', 387), ('1', 114)]]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10:48:10', [('0', 375), ('1', 126)]]\n",
      "['10:48:41', [('0', 378), ('1', 123)]]\n",
      "['10:49:11', [('0', 391), ('1', 110)]]\n",
      "['10:49:43', [('0', 385), ('1', 116)]]\n"
     ]
    }
   ],
   "source": [
    "# Check reduced by key (sentiment evaluation) results for each timestamp\n",
    "res_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ends real-time streaming\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the gathered data above, we can see that Naive Bayes text classifier model predicts that, for each 500 tweets per batch interval, approximately 370 (74%) of the tweets related to Trump's twitter account are negative and therefore, 130 (26%) are with positive sentiments. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
